{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "solved-prison",
   "metadata": {
    "id": "KaqO0VSvmdF6"
   },
   "source": [
    "# Mendefinisikan Variable Penting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "committed-separation",
   "metadata": {
    "id": "beginning-printing"
   },
   "outputs": [],
   "source": [
    "# Variable Penting\n",
    "# ListLabelUnique = []\n",
    "# ListLabelSpesifikUnique = []\n",
    "testSize = 0.15 #Pembagian ukuran datatesing\n",
    "\n",
    "MAX_NB_WORDS = 100000 #Maximum jumlah kata pada vocabulary yang akan dibuat\n",
    "max_seq_len = 0 #Panjang kalimat maximum\n",
    "\n",
    "num_epochs = 400 #Jumlah perulangan / epoch saat proses training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-lobby",
   "metadata": {
    "id": "T8xDHlDCnBVw"
   },
   "source": [
    "# Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "phantom-hamburg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "progressive-immunology",
    "outputId": "4613be8a-5fe0-49e2-96e8-c5145a3645f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import CSV\n",
    "import csv\n",
    "# Import json\n",
    "import json\n",
    "# Import Pandas\n",
    "import pandas as pd\n",
    "# Settingan di Pandas untuk menghilangkan warning\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# Import Numpy\n",
    "import numpy as np\n",
    "\n",
    "# Loading Bar TQDM\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Stopword Removal\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Stemming (Sastrawi)\n",
    "# !pip install Sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# Tokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# pad_sequences untuk google colab\n",
    "# from keras.utils import pad_sequences\n",
    "# pad_sequences untuk jupter-lab\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pickle FastText\n",
    "import pickle\n",
    "\n",
    "# Split Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Label Encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Model Building\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from keras.models import load_model\n",
    "\n",
    "# Callbacks\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Plot Model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# Grafik\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(0)\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-contract",
   "metadata": {
    "id": "U17g4JBInS_B"
   },
   "source": [
    "# Function Mengubah File CSV menjadi File JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adaptive-vancouver",
   "metadata": {
    "id": "alpine-access"
   },
   "outputs": [],
   "source": [
    "# def csv_to_json(CsvPath, JsonPath):\n",
    "#     JsonArray = []\n",
    "# #     UniqueLabel = {}\n",
    "    \n",
    "#     # Membuka File CSV \n",
    "#     with open(CsvPath, encoding='utf-8') as CsvFile: \n",
    "#         # Membaca file csv dan menjadikannya dict menggunakan module DictReader\n",
    "#         CsvReader = csv.DictReader(CsvFile)\n",
    "\n",
    "#         for row in CsvReader:\n",
    "#             # Memasukkan tiap baris dict ke dalam array\n",
    "            \n",
    "#             # Menambahkan label spesifik\n",
    "# #             if(row['Label'] not in UniqueLabel):\n",
    "# #                 UniqueLabel[row['Label']]=1\n",
    "# #                 ListLabelUnique.append(row['Label'])\n",
    "# #             else:\n",
    "# #                 UniqueLabel[row['Label']]+=1\n",
    "                            \n",
    "# #             row['Label_Spesifik']=row['Label']+\"_\"+str(UniqueLabel[row['Label']])\n",
    "#             # print(row['LabelSpesifik'])\n",
    "#             JsonArray.append(row)\n",
    "            \n",
    "                            \n",
    "#     # Memasukkan dict ke dalam json\n",
    "#     with open(JsonPath, 'w', encoding='utf-8') as JsonFile: \n",
    "#         JsonString = json.dumps(JsonArray, indent=4)\n",
    "#         JsonFile.write(JsonString)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-pencil",
   "metadata": {
    "id": "KvnLcP-yoT7h"
   },
   "source": [
    "# Memanggil Function csv_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "molecular-denmark",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "id": "HdwT3kNCoRtR",
    "outputId": "7066ef07-f816-4f4f-9879-f5ab6e6475db"
   },
   "outputs": [],
   "source": [
    "# csvFilePath = r'D:\\Kuliah\\File Kuliah\\Skripsi\\Program Chabot\\MelekSaham\\Dataset\\Intents.csv'\n",
    "# jsonFilePath = r'D:\\Kuliah\\File Kuliah\\Skripsi\\Program Chabot\\MelekSaham\\Dataset\\Intents.json'\n",
    "# csv_to_json(csvFilePath, jsonFilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-hughes",
   "metadata": {
    "id": "lHgDiyDBnh-Y"
   },
   "source": [
    "# Function Membaca file JSON menjadi DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "focused-amateur",
   "metadata": {
    "id": "interim-houston"
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    # Membaca file intents.json untuk dijadikan Dataframe\n",
    "#     df = pd.read_json(filename) \n",
    "    df = pd.read_csv(filename) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-floor",
   "metadata": {
    "id": "eH6MTlx-n16I"
   },
   "source": [
    "# Memanggil Function read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "packed-jacket",
   "metadata": {
    "id": "ethical-andrew"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pertanyaan</th>\n",
       "      <th>Jawaban</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apa perbedaan antara investasi jangka pendek d...</td>\n",
       "      <td>Investasi jangka pendek biasanya berlangsung d...</td>\n",
       "      <td>Investasi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bagaimana cara mengelola risiko dalam investas...</td>\n",
       "      <td>Beberapa cara untuk mengelola risiko dalam inv...</td>\n",
       "      <td>Investasi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apa yang harus diperhatikan saat memilih saham...</td>\n",
       "      <td>Saat memilih saham untuk investasi perhatikan ...</td>\n",
       "      <td>Investasi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bagaimana cara mengukur kinerja investasi saham?</td>\n",
       "      <td>Kinerja investasi saham dapat diukur dengan me...</td>\n",
       "      <td>Investasi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bagaimana cara memahami risiko investasi saham?</td>\n",
       "      <td>Memahami risiko investasi saham melibatkan pen...</td>\n",
       "      <td>Investasi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>Bagaimana menghitung nilai intrinsik dengan me...</td>\n",
       "      <td>Untuk menghitung nilai intrinsik dengan metode...</td>\n",
       "      <td>Strategi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>Apa itu Margin of Safety dalam valuasi saham?</td>\n",
       "      <td>Margin of Safety adalah konsep penting bagi in...</td>\n",
       "      <td>Istilah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>Apa yang dimaksud dengan undervalued (dihargai...</td>\n",
       "      <td>Undervalued dalam konteks penilaian saham adal...</td>\n",
       "      <td>Istilah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>Apa yang dimaksud dengan overvalued (dihargai ...</td>\n",
       "      <td>Overvalued dalam konteks penilaian saham adala...</td>\n",
       "      <td>Istilah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>Bagaimana cara menentukan apakah suatu saham u...</td>\n",
       "      <td>Cara menentukan apakah suatu saham undervalued...</td>\n",
       "      <td>Strategi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Pertanyaan  \\\n",
       "0    Apa perbedaan antara investasi jangka pendek d...   \n",
       "1    Bagaimana cara mengelola risiko dalam investas...   \n",
       "2    Apa yang harus diperhatikan saat memilih saham...   \n",
       "3     Bagaimana cara mengukur kinerja investasi saham?   \n",
       "4      Bagaimana cara memahami risiko investasi saham?   \n",
       "..                                                 ...   \n",
       "360  Bagaimana menghitung nilai intrinsik dengan me...   \n",
       "361      Apa itu Margin of Safety dalam valuasi saham?   \n",
       "362  Apa yang dimaksud dengan undervalued (dihargai...   \n",
       "363  Apa yang dimaksud dengan overvalued (dihargai ...   \n",
       "364  Bagaimana cara menentukan apakah suatu saham u...   \n",
       "\n",
       "                                               Jawaban      Label  \n",
       "0    Investasi jangka pendek biasanya berlangsung d...  Investasi  \n",
       "1    Beberapa cara untuk mengelola risiko dalam inv...  Investasi  \n",
       "2    Saat memilih saham untuk investasi perhatikan ...  Investasi  \n",
       "3    Kinerja investasi saham dapat diukur dengan me...  Investasi  \n",
       "4    Memahami risiko investasi saham melibatkan pen...  Investasi  \n",
       "..                                                 ...        ...  \n",
       "360  Untuk menghitung nilai intrinsik dengan metode...   Strategi  \n",
       "361  Margin of Safety adalah konsep penting bagi in...    Istilah  \n",
       "362  Undervalued dalam konteks penilaian saham adal...    Istilah  \n",
       "363  Overvalued dalam konteks penilaian saham adala...    Istilah  \n",
       "364  Cara menentukan apakah suatu saham undervalued...   Strategi  \n",
       "\n",
       "[365 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Memanggil Function Read Data\n",
    "FileIntents = './Dataset/Intents.csv'\n",
    "df = read_data(FileIntents)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "understood-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat fungsi label encoder\n",
    "def encode_label(df):\n",
    "    # Encoding Categorical Data (Mengubah data kategorikal menjadi angka)\n",
    "    LE = LabelEncoder()\n",
    "    #df['Label_Spesifik_Encoded'] = LE.fit_transform(df['Label_Spesifik'])\n",
    "    df['Label_Encoded'] = LE.fit_transform(df['Label'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "announced-knife",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Label_Encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IPO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Investasi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Istilah</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Strategi</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trading</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label  Label_Encoded\n",
       "0        IPO              0\n",
       "1  Investasi              1\n",
       "2    Istilah              2\n",
       "3   Strategi              3\n",
       "4    Trading              4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menampilkan Label sebelum dan sesudah encoded\n",
    "df = encode_label(df)\n",
    "pd.set_option(\"max_rows\", None)\n",
    "# df[['Label_Spesifik','Label_Spesifik_Encoded']].sort_values([\"Label_Spesifik\"],ascending=[True])\n",
    "label = df[['Label','Label_Encoded']].sort_values([\"Label_Encoded\"],ascending=[True]).drop_duplicates().reset_index(drop=True)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "reasonable-slovak",
   "metadata": {
    "id": "mysterious-firewall",
    "outputId": "ec65c6e6-4441-485c-e102-e95989637dbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape dari Label_Spesifik_Encoded\n",
    "pd.get_dummies(df['Label']).values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "integral-drink",
   "metadata": {
    "id": "built-shipping"
   },
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    # Case Folding\n",
    "    data['lower'] = data['Pertanyaan'].str.lower()\n",
    "    \n",
    "    # Punctual Removal\n",
    "    data['punctual'] = data['lower'].str.replace('[^a-zA-Z0-9]+',' ', regex=True)\n",
    "    \n",
    "    # Normalization\n",
    "    kamus_baku = pd.read_csv('kata_baku.csv', sep=\";\")\n",
    "    dict_kamus_baku = kamus_baku[['slang','baku']].to_dict('list')\n",
    "    dict_kamus_baku = dict(zip(dict_kamus_baku['slang'], dict_kamus_baku['baku']))\n",
    "    norm = []\n",
    "    for i in data['punctual']:\n",
    "        res = \" \".join(dict_kamus_baku.get(x, x) for x in str(i).split())\n",
    "        norm.append(str(res))\n",
    "    data['normalize'] = norm\n",
    "    \n",
    "    # Stopword Removal\n",
    "    stop_words = set(stopwords.words('indonesian'))\n",
    "    swr = []\n",
    "    for i in tqdm(data['normalize']):\n",
    "        tokens = word_tokenize(i)\n",
    "        filtered = [word for word in tokens if word not in stop_words]\n",
    "        swr.append(\" \".join(filtered))\n",
    "    data['stopwords'] = swr\n",
    "    \n",
    "    # Stemming\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    stem = []\n",
    "    for i in tqdm(data['stopwords']):\n",
    "#     for i in tqdm(data['stopwords']):\n",
    "        stem.append(stemmer.stem(str(i)))\n",
    "    data['stemmed'] = stem\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "great-botswana",
   "metadata": {
    "id": "southwest-exclusion",
    "outputId": "321a5215-39fe-4ab0-eb2a-446265f5fb08"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 365/365 [00:00<00:00, 6403.14it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 365/365 [00:21<00:00, 17.34it/s]\n"
     ]
    }
   ],
   "source": [
    "df = preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "photographic-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data splitting\n",
    "df_training, df_testing = train_test_split(df, test_size=testSize, random_state=42, shuffle=True)\n",
    "df_training = df_training.reset_index(drop=True)\n",
    "df_testing = df_testing.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "removable-yeast",
   "metadata": {
    "id": "brave-shock",
    "outputId": "10608590-aeff-41e4-d98b-29c4c3577a16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pengaruh faktor ekonomi global pasar saham investasi saham\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# Mengecek panjang kalimat maksimum\n",
    "longest_string = max(df['stemmed'].values.tolist(), key=len)\n",
    "max_seq_len = len(longest_string.split())\n",
    "pickle.dump(max_seq_len, open('max_seq_len.pkl','wb'))\n",
    "print(longest_string)\n",
    "print(max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "danish-excess",
   "metadata": {
    "id": "established-matter"
   },
   "outputs": [],
   "source": [
    "def tokenize_corpus(data_corpus):\n",
    "    global corpus_tokenizer\n",
    "    corpus_tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "    corpus_tokenizer.fit_on_texts(data_corpus['stemmed'])\n",
    "    corpus_sequences = corpus_tokenizer.texts_to_sequences(data_corpus['stemmed'])\n",
    "    corpus_word_index = corpus_tokenizer.word_index\n",
    "\n",
    "    return corpus_sequences, corpus_word_index\n",
    "\n",
    "def tokenize_training(data_training):\n",
    "    global model_tokenizer #Menggunakan variabel global agar 'tokenizer' bisa dipake di luar fungsi ini\n",
    "    model_tokenizer = Tokenizer(oov_token = \"<OOV>\")\n",
    "    model_tokenizer.fit_on_texts(data_training['stemmed'])\n",
    "    model_word_index = model_tokenizer.word_index\n",
    "    # Integer Tokenizer\n",
    "    train_sequences = model_tokenizer.texts_to_sequences(data_training['stemmed'])\n",
    "    # Padding\n",
    "    train_sequences_padded = pad_sequences(train_sequences, maxlen = max_seq_len)\n",
    "\n",
    "    return train_sequences, train_sequences_padded, model_word_index\n",
    "\n",
    "def tokenize_testing(data_testing):\n",
    "    test_model_sequences = model_tokenizer.texts_to_sequences(data_testing['stemmed'])\n",
    "    test_model_sequences_padded = pad_sequences(test_model_sequences, maxlen = max_seq_len)\n",
    "    test_corpus_sequences = corpus_tokenizer.texts_to_sequences(data_testing['stemmed'])\n",
    "    return test_model_sequences, test_model_sequences_padded, test_corpus_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "arctic-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test oov_token\n",
    "# tokenizer = Tokenizer(num_words = MAX_NB_WORDS, char_level=False)\n",
    "# tokenizer.fit_on_texts(df['stemmed'])\n",
    "# word_index = tokenizer.word_index\n",
    "# # Integer Tokenizer\n",
    "# coba = tokenizer.texts_to_sequences([['saham', 'bla', 'bagaimana','sakdjsadkasd']])\n",
    "# coba = pad_sequences(coba, maxlen = max_seq_len)\n",
    "# coba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "genetic-joshua",
   "metadata": {
    "id": "pharmaceutical-programming",
    "outputId": "94fdd9b6-dde5-4890-8082-652d225b497b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebelum Tokenizer : profitability ratio\n",
      "Setelah Tokenizer : [  0   0   0   0   0   0 154   9]\n"
     ]
    }
   ],
   "source": [
    "corpus_sequences, corpus_word_index = tokenize_corpus(df)\n",
    "train_sequences, train_sequences_padded, model_word_index  = tokenize_training(df_training)\n",
    "test_model_sequences, test_model_sequences_padded, test_corpus_sequences = tokenize_testing(df_testing)\n",
    "print(\"Sebelum Tokenizer :\" , df_training['stemmed'][0])\n",
    "print(\"Setelah Tokenizer :\" , train_sequences_padded[0])\n",
    "# print(\"Model Word Index :\" , model_word_index)\n",
    "# print(\"Corpus Word Index :\" , corpus_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alasan sequence model untuk predict berbentuk 2 dimensional array\n",
    "print(corpus_tokenizer.texts_to_sequences(['idx30']))\n",
    "print(corpus_tokenizer.texts_to_sequences('idx30'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Tokenized Word :\",len(list(model_word_index.keys())))\n",
    "print(\"Corpus Tokenized Word :\",len(list(corpus_word_index.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-gentleman",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-sight",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(model_tokenizer, open('model_tokenizer.pkl','wb'))\n",
    "pickle.dump(corpus_tokenizer, open('corpus_tokenizer.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(model_word_index, open('model_word_index.pkl','wb'))\n",
    "pickle.dump(corpus_word_index, open('corpus_word_index.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pertanyaan df_training setelah di \n",
    "df_training = df_training.reset_index(drop=True) #Reset Index\n",
    "# df_training['Sequences'] = word_seq_train.tolist()\n",
    "df_training['Sequences'] = train_sequences\n",
    "# df_training[['stemmed','Sequences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pertanyaan df_testing setelah di tokenize\n",
    "df_testing = df_testing.reset_index(drop=True) #Reset Index\n",
    "# df_testing['Sequences'] = test_sequences_padded.tolist()\n",
    "df_testing['Model_Sequences'] = test_model_sequences\n",
    "df_testing['Corpus_Sequences'] = test_corpus_sequences\n",
    "# df_testing[['stemmed','Sequences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pertanyaan df setelah di tokenize\n",
    "# df['Sequences'] = word_seq_corpus.tolist()\n",
    "df['Sequences'] = corpus_sequences\n",
    "# df[['stemmed','Sequences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-garbage",
   "metadata": {
    "id": "successful-penny"
   },
   "outputs": [],
   "source": [
    "# Pendefenisian fungsi word embedding\n",
    "def word_embedding():\n",
    "    fasttext_word_to_index = pickle.load(open(\"fasttext_voc\", 'rb'))\n",
    "    \n",
    "    words_not_found = []\n",
    "    nb_words = min(MAX_NB_WORDS, len(model_word_index)+1) # Word_index berasal dari tokenize\n",
    "#     nb_words = min(MAX_NB_WORDS, len(model_word_index)) # Word_index berasal dari tokenize\n",
    "\n",
    "    embed_dim = 300 # dimensi matrix (dari fastTextnya cc.id.300.vec)\n",
    "    embedding_matrix = np.zeros((nb_words, embed_dim)) # Membuat array vector berisi 0 (202 baris kebawah,300 dimension kesamping)\n",
    "    # Hanya 134 yang akan terisi (dari index 1 sampai 134) untuk index ke 0 hanya akan berisi 300 buah dimension bernilai 0\n",
    "\n",
    "    for word, index in model_word_index.items():\n",
    "        if index < nb_words:\n",
    "            embedding_vector = fasttext_word_to_index.get(word) # mengambil vector word yang dimasukkan dari word_index dari fasttext_voc\n",
    "            if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "                embedding_matrix[index] = embedding_vector # memasukkan vector dari word yang didapatkan ke dalam index matrix yang sama dengan index dari word_index\n",
    "            else:\n",
    "                words_not_found.append(word)\n",
    "    \n",
    "    return embedding_matrix, nb_words, embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-shooting",
   "metadata": {
    "id": "supreme-visitor"
   },
   "outputs": [],
   "source": [
    "# Pemanggilan Fungsi word embedding\n",
    "embedding_matrix, nb_words, embed_dim = word_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-destiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-mining",
   "metadata": {
    "id": "corresponding-recycling",
    "outputId": "d3d18bac-4e98-45cd-a3b6-a55047691756"
   },
   "outputs": [],
   "source": [
    "# Penentuan X (input) dan Y (output)\n",
    "X_train = train_sequences_padded\n",
    "X_test = test_model_sequences_padded\n",
    "\n",
    "# One Hot Encoded Label\n",
    "Y_train = pd.get_dummies(df_training['Label']).values\n",
    "Y_test = pd.get_dummies(df_testing['Label']).values\n",
    "\n",
    "# Integer Encoded Label\n",
    "# Y_train = df_training['Label_Encoded'].values\n",
    "# Y_test = df_testing['Label_Encoded'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-astronomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-plain",
   "metadata": {
    "id": "express-speech",
    "outputId": "0a2a7b1f-9cf1-461a-e7c6-7b3773f26fa7"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "#         keras.layers.Embedding(nb_words, embed_dim ,input_length=max_seq_len, \n",
    "#                                weights=[embedding_matrix], trainable=False),\n",
    "        keras.layers.Embedding(nb_words,100,input_length=max_seq_len),\n",
    "#         keras.layers.Conv1D(256, 5, padding='same', activation='relu'),\n",
    "#         keras.layers.MaxPooling1D(pool_size=4),\n",
    "#         keras.layers.LSTM(10),\n",
    "#         keras.layers.Dense(1, activation = 'softmax') #softmax\n",
    "#         keras.layers.LSTM(128, return_sequences = True),\n",
    "#         keras.layers.LSTM(64, return_sequences = True),\n",
    "#         keras.layers.Dropout(0.5),\n",
    "        keras.layers.LSTM(32),# returns a sequence of vectors of dimension 32 ini\n",
    "#         keras.layers.Dropout(0.5),\n",
    "#         keras.layers.Dropout(0.3),\n",
    "#         keras.layers.LSTM(32, return_sequences=True), # returns a sequence of vectors of dimension 32\n",
    "#         keras.layers.LSTM(10),  # return a single vector of dimension 32\n",
    "#         keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dense(len(Y_train[0]), activation='softmax')\n",
    "    ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-kidney",
   "metadata": {
    "id": "metric-principal",
    "outputId": "0d4f9f5a-0429-4f8f-c298-26afb5c9299d"
   },
   "outputs": [],
   "source": [
    "# # # Output Embedding & LSTM Layer\n",
    "# model.compile(optimizer='adam', metrics=['accuracy'])\n",
    "# print(model.predict([[0,0,0,0,0,0,23,2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-cutting",
   "metadata": {
    "id": "metric-principal",
    "outputId": "0d4f9f5a-0429-4f8f-c298-26afb5c9299d"
   },
   "outputs": [],
   "source": [
    "# One Hot Label Encoded\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto', patience=3)\n",
    "chatbotmodel = model.fit(X_train, Y_train,\n",
    "        epochs = 300, \n",
    "        callbacks = [es],\n",
    "        validation_split=0.15,\n",
    "        verbose = True # Verbose = 0 (tidak nampak progress), 1/True (progress bar), 2 (angka)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Integer Label Encoded\n",
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto', patience=5)\n",
    "# chatbotmodel = model.fit(X_train, Y_train,\n",
    "#         epochs = 300, \n",
    "#         callbacks = [es],\n",
    "#         validation_split=0.15,\n",
    "#         verbose = True # Verbose = 0 (tidak nampak progress), 1/True (progress bar), 2 (angka)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model,to_file='model.png',show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('chatbotmodel.h5', chatbotmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-louisville",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(chatbotmodel.history['loss'], lw=2.0, color='b', label='train')\n",
    "plt.plot(chatbotmodel.history['val_loss'], lw=2.0, color='r', label='val')\n",
    "plt.title(\"Loss History\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-generic",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(chatbotmodel.history['accuracy'], lw=2.0, color='b', label='train')\n",
    "plt.plot(chatbotmodel.history['val_accuracy'], lw=2.0, color='r', label='val')\n",
    "plt.title(\"Accuracy History\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(model,test_sequences_padded):\n",
    "    categorical_predicted_label = []\n",
    "    onehot_predicted_label = model.predict(test_sequences_padded)\n",
    "    for i in range(0,len(test_sequences_padded)):\n",
    "        categorical_predicted_label.append(onehot_predicted_label[i].argmax())\n",
    "    return onehot_predicted_label, categorical_predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-custody",
   "metadata": {
    "id": "moral-continent",
    "outputId": "5bdbce71-1352-4f57-a171-1d612cb6ab8e"
   },
   "outputs": [],
   "source": [
    "# Predict test data\n",
    "# predict_test = model.predict(test_sequences_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-karma",
   "metadata": {
    "id": "integral-shower"
   },
   "outputs": [],
   "source": [
    "# Convert hasil predict_test dari one hot label menjadi integer categorical label \n",
    "# integer_categorical_label = []\n",
    "# for i in range(0,len(test_sequences_padded)):\n",
    "#     array.append(predict_test[i].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-engagement",
   "metadata": {
    "id": "systematic-clothing",
    "outputId": "5c140a20-c5ac-472f-ed89-c8b780bdcbf0"
   },
   "outputs": [],
   "source": [
    "# pd.set_option(\"max_rows\", None)\n",
    "# df['Predicted'] = array\n",
    "# df[['Label_Spesifik_Encoded','Predicted']].loc[df['Label_Spesifik_Encoded'] != df['Predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-evanescence",
   "metadata": {
    "id": "systematic-clothing",
    "outputId": "5c140a20-c5ac-472f-ed89-c8b780bdcbf0"
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"max_rows\", None)\n",
    "onehot_predicted_label, df_testing['Predicted_Label'] = predict_test(model,test_model_sequences_padded)\n",
    "df_testing[['Pertanyaan','Label_Encoded','Predicted_Label']].loc[df_testing['Label_Encoded'] != df_testing['Predicted_Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-momentum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(data, labels, output_filename):\n",
    "    \"\"\"Plot confusion matrix using heatmap.\n",
    " \n",
    "    Args:\n",
    "        data (list of list): List of lists with confusion matrix data.\n",
    "        labels (list): Labels which will be plotted across x and y axis.\n",
    "        output_filename (str): Path to output file.\n",
    " \n",
    "    \"\"\"\n",
    "    seaborn.set(color_codes=True)\n",
    "    plt.figure(1, figsize=(9, 6))\n",
    " \n",
    "    plt.title(\"Confusion Matrix\")\n",
    " \n",
    "    seaborn.set(font_scale=1.4)\n",
    "    ax = seaborn.heatmap(data, annot=True, cmap=\"YlGnBu\", cbar_kws={'label': 'Scale'})\n",
    " \n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    " \n",
    "    ax.set(ylabel=\"True Label\", xlabel=\"Predicted Label\")\n",
    " \n",
    "    plt.savefig(output_filename, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "cm = confusion_matrix(df_testing[['Label_Encoded']], df_testing[['Predicted_Label']])\n",
    "plot_confusion_matrix(cm, ['IPO','Investasi','Istilah','Strategi','Trading'], 'confusion_matrix.png')\n",
    "# sn.heatmap(cm, annot=True, xticklabels=['IPO','Investasi','Istilah','Strategi','Trading'], yticklabels=['IPO','Investasi','Istilah','Strategi','Trading'], annot_kws={\"size\": 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_testing.iloc[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing[['Label_Encoded','Predicted_Label']].loc[df_testing['Label_Encoded'] != df_testing['Predicted_Label']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-federation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,len(word_seq_test)):\n",
    "#     for j in word_seq_test[i]:\n",
    "#         print(j)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_testing[['Label_Encoded','Predicted']].sort_values([\"Predicted\"],ascending=[True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pembagian dataframe_corpus berdasarkan label untuk matching\n",
    "df_IPO = df[['Pertanyaan','Jawaban','Sequences']].loc[df['Label_Encoded']==int(0)].reset_index(drop=True)\n",
    "df_Investasi = df[['Pertanyaan','Jawaban','Sequences']].loc[df['Label_Encoded']==int(1)].reset_index(drop=True)\n",
    "df_Istilah = df[['Pertanyaan','Jawaban','Sequences']].loc[df['Label_Encoded']==int(2)].reset_index(drop=True)\n",
    "df_Strategi = df[['Pertanyaan','Jawaban','Sequences']].loc[df['Label_Encoded']==int(3)].reset_index(drop=True)\n",
    "df_Trading = df[['Pertanyaan','Jawaban','Sequences']].loc[df['Label_Encoded']==int(4)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df_IPO, open('df_IPO.pkl','wb'))\n",
    "pickle.dump(df_Investasi, open('df_Investasi.pkl','wb'))\n",
    "pickle.dump(df_Istilah, open('df_Istilah.pkl','wb'))\n",
    "pickle.dump(df_Strategi, open('df_Strategi.pkl','wb'))\n",
    "pickle.dump(df_Trading, open('df_Trading.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Investasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pembagian dataframe_testing berdasarkan predicted label untuk matching\n",
    "def matching_testing(df_testing):\n",
    "    Prediksi_Jawaban = []\n",
    "    for row in range(len(df_testing)):\n",
    "        #Mengambil Predicted_Label per baris\n",
    "        Predicted_Label = df_testing['Predicted_Label'].iloc[row] \n",
    "        # Menentukan df yang dipakai untuk matching\n",
    "        if(Predicted_Label==0):\n",
    "            check_df = df_IPO\n",
    "        elif(Predicted_Label==1):\n",
    "            check_df = df_Investasi\n",
    "        elif(Predicted_Label==2):\n",
    "            check_df = df_Istilah\n",
    "        elif(Predicted_Label==3):\n",
    "            check_df = df_Strategi\n",
    "        elif(Predicted_Label==4):\n",
    "            check_df = df_Trading\n",
    "    \n",
    "        Compatibility = [0]*len(check_df)\n",
    "        \n",
    "        # Looping tiap baris Sequences check_df\n",
    "        index = 0\n",
    "        for check_sequences in check_df['Sequences']:\n",
    "            # Looping tiap element Sequences df testing per baris\n",
    "            for element in df_testing['Corpus_Sequences'].iloc[row]:\n",
    "                if(element in check_sequences):\n",
    "                    Compatibility[index]+=1\n",
    "#                     print(Compatibility)\n",
    "            Compatibility[index] = Compatibility[index]/len(df_testing['Corpus_Sequences'].iloc[row])\n",
    "            index += 1\n",
    "    \n",
    "        index_max_compatibility = []\n",
    "        for idx, value in enumerate(Compatibility):\n",
    "            if value == max(Compatibility):\n",
    "                index_max_compatibility.append(idx)\n",
    "\n",
    "        perfect_compatibilty_sequence = []\n",
    "        perfect_compatibilty_index = 0\n",
    "        for idx in index_max_compatibility:\n",
    "            if (idx == index_max_compatibility[0]):\n",
    "                perfect_compatibilty_sequence = check_df['Sequences'].iloc[idx]\n",
    "                perfect_compatibilty_index = idx\n",
    "            else:\n",
    "                if(len(check_df['Sequences'].iloc[idx]) <= len(perfect_compatibilty_sequence)):\n",
    "                    perfect_compatibilty_sequence = check_df['Sequences'].iloc[idx]\n",
    "                    perfect_compatibilty_index = idx\n",
    "\n",
    "        Prediksi_Jawaban.append(check_df['Jawaban'].iloc[perfect_compatibilty_index])\n",
    "#         print(Prediksi_Jawaban)\n",
    "    return Prediksi_Jawaban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediksi_Jawaban = matching_testing(df_testing)\n",
    "df_testing['Prediksi_Jawaban'] = Prediksi_Jawaban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-fisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing[['Pertanyaan','Jawaban','Prediksi_Jawaban']].loc[df_testing['Jawaban']!=df_testing['Prediksi_Jawaban']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing[['Pertanyaan','Jawaban','Prediksi_Jawaban']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict([[0,0,0,0,0,0,0,0,0,0,3,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(np.array([[3,2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Istilah['Sequences'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Istilah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df['Label_Encoded']==int(2)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([[1,1,1,1]][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-organ",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
